## Concurrency introduction #ostep/26 
*Overlap of **I/O** and other activities within a single process*
Similar to processes
- Own #PC and registers
- #CPU performs context switches
- Where processes saved to a (**READ AND ADD NOTES ON THIS TO PREVIOUS LECTURE. PROCESS CONTROL BLOCK**.) #PCB, threads use *thread control blocks* #TCB
![](Pasted%20image%2020240424082546.png) ![](Pasted%20image%2020240424083125.png)
Differ in stacks
![](Pasted%20image%2020240421181532.png)
###### Uncontrolled Scheduling #ostep/26/5
Given two threads incrementing the same variable, it may only be incremented once
![](Pasted%20image%2020240423203703.png)
A *race condition*
- Specifically a *data race*
- Area with possible *race condidition* is a *critical section*
## Thread API #ostep/27 
*uses* `#include <pthread.h>` *unless otherwise specified*

|                     |                                                                                                                                                                                                             |
| ------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Thread creation     | `{c}int pthread_create(pthread_t *thread, const pthread_attr_t *attr, void *(*start_routine)(void*), void *arg);`                                                                                           |
| Thread completion   | `{c}int pthread_join(pthread_t thread, void **value_ptr)`                                                                                                                                                   |
| Locks               | `{c}int pthread_mutex_lock(pthread_mutex_t *mutex);`<br>`{c}int pthread_mutex_unlock(pthread_mutex_t *mutex);`<br>`{c}pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;;`<br>`{sh}man 3 pthread_mutex_lock` |
| Condition variables | `{c}int pthread_cond_wait(pthread_cond_t *cond, pthread_mutex_t *mutex);`<br>`{c}int pthread_cond_signal(pthread_cond_t *cond);`<br>`{c}pthread_cond_t cond = PTHREAD_COND_INITIALIZER;`                    |
|                     |                                                                                                                                                                                                             |
## Locks #ostep/28 
Goals #ostep/28/4:
- Mutual exclusion
- fairness
	- Do not *starve* waiting threads
- Performance

#### Building a lock #ostep/28/5 #ostep/28/6 #ostep/28/7 #ostep/28/8 #ostep/28/9 #ostep/28/10 #ostep/28/11 #ostep/28/12 #ostep/28/13 #ostep/28/14 #ostep/28/15 #ostep/28/16 
###### Turning off interrupts #ostep/28/5 
First approach, so bad
- Program can take over machine
###### Spin locks & Test-And-Set #ostep/28/7 
*Many names -* #x86-64 *atomic exchange, `xchg` instruction*
- Single instruction to set value and return old value
- Default value 0
- On lock, set 1 and read 0
- If locked, on lock set 1 but read 1
	- Go into spin (wait)
- Requires *preemptive scheduling* on #CPU to allow waiting thread to check again later

However, evaluating #ostep/28/8 
- No fairness guarantee. Locked thread may never come back
- bad performance on single cpu, as all wait time while checking is lost.
#ostep/28/9 #ostep/28/10 #ostep/28/11 are all variations on *Test-And-Set*

###### Locks without spinning #ostep/28/12 
*Locking without wasting $\frac{N-1}{N}$ of our cycles*

#ostep/28/13 Yielding
- `{c}yield()` when blocked by lock, giving away control
- Still starves, and with many threads is not performant

#ostep/28/14 Parking (sleeping) in queue **Note**: #SolarisOS, not #LINUX
- `{c}park()` when blocked after adding to queue
- `{c}unpark(int pid)` when unlocking to pass on two next in line
- Still uses spinlock for queue lock (guard). Only few instructions though.

#ostep/28/15 Sleeping in queue, #linux style
- `{c}futex_wait(mutex, int v)` to wait for access
- `{c}futex_wake(mutex)` to release and give back
#ostep/28/16 `{c}futex` is a two-phase approach. Faster may be to spin a bit, and then sleep

## Lock-based Concurrent Data Structures #ostep/29
*Creating datastructures for concurrent use*
#### Concurrent Counters #ostep/29/1
*Approximate counter*
![](Pasted%20image%2020240423213952.png)
- Create counter for each CPU
	- `{c}int cpu = threadid % NUMCPUS`
- Schedule updates to pull value to global
![](Pasted%20image%2020240423214257.png)
- *Approximation Factor* $(S)$ is how often global is updated
	- More often -> more locks
#### Linked lists #ostep/29/2
List holds lock and locks critical section
- More effective if every $N$ node has lock
	- *Hand-over-hand locking*
```c
void List_Init(list_t *L) {
	L->head = NULL;
	pthread_mutex_init(&L->lock, NULL);
}

int List_Insert(list_t *L, int key) {
// synchronization not needed
	node_t *new = malloc(sizeof(node_t));
	if (new == NULL) {
		perror("malloc");
		return -1;
	}
	new->key = key;
	// just lock critical section
	pthread_mutex_lock(&L->lock);
	new->next = L->head;
	L->head = new;
	pthread_mutex_unlock(&L->lock);
	return 0; // success
}

int List_Lookup(list_t *L, int key) {
	int rv = -1;
	pthread_mutex_lock(&L->lock);
	node_t *curr = L->head;
	while (curr) {
	if (curr->key == key) {
		rv = 0;
		break;
		}
		curr = curr->next;
	}
	pthread_mutex_unlock(&L->lock);
	return rv; // now both success and failure
}
```
#### Queue #ostep/29/3
Lock for head and tail, with dummy node to separate at start
#### Hash Table #ostep/29/4
Just an array of concurrent lists
- No lock on table needed. Only read from.
```c
#define BUCKETS (101)

typedef struct __hash_t {
	list_t lists[BUCKETS];
} hash_t;

void Hash_Init(hash_t *H) {
	int i;
	for (i = 0; i < BUCKETS; i++)
		List_Init(&H->lists[i]);
}

int Hash_Insert(hash_t *H, int key) {
	return List_Insert(&H->lists[key % BUCKETS], key);
}

int Hash_Lookup(hash_t *H, int key) {
	return List_Lookup(&H->lists[key % BUCKETS], key);
}
```
![](Pasted%20image%2020240423215442.png)
